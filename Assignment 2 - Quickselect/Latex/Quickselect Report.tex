\documentclass[
12pt,
a4paper,
oneside,
headinclude,
footinclude]{article}



\usepackage[table,xcdraw,svgnames]{xcolor}
\usepackage[capposition=bottom]{floatrow}
\usepackage[colorlinks]{hyperref} % to add hyperlinks
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{csquotes}
\usepackage{amsmath} % For the big bracket
\usepackage[export]{adjustbox}[2011/08/13]
% \usepackage{subfig}
\usepackage{array}
\usepackage{url}
\usepackage{graphicx} % to insert images
\usepackage{titlepic} % to insert image on front page
\usepackage{geometry} % to define margin
\usepackage{listings} % to add code
\usepackage{caption}
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage[utf8]{inputenc} % Required for including letters with accents
\usepackage{color}
\usepackage{subcaption}
\usepackage[nochapters, eulermath, dottedtoc ]{classicthesis}
\usepackage{listings} % For R code

\usepackage{minted} % For Rust code

\usepackage{color}

\usemintedstyle{tango}

\usepackage{etoolbox}


\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}


\lstset{frame=tb,
  language=R,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3,
  frame=none
}

\definecolor{webbrown}{rgb}{.6,0,0}

\usepackage{titlesec} % to customize titles
\titleformat{\chapter}{\normalfont\huge}{\textbf{\thechapter.}}{20pt}{\huge\textbf}[\vspace{2ex}\titlerule] % to customize chapter title aspect
\titleformat{\section} % to customize section titles
  {\fontsize{14}{15}\bfseries}{\thesection}{1em}{}

\titlespacing*{\chapter}{0pt}{-50pt}{20pt} % to customize chapter title space

\graphicspath{ {../Figures/} } % images folder
\parindent0pt \parskip10pt % make block paragraphs
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=3cm,rmargin=3cm,headheight=3cm,headsep=3cm,footskip=1cm} % define margin
\hyphenation{Fortran hy-phen-ation}

\AtBeginDocument{%
    \hypersetup{
    colorlinks=true, breaklinks=true, bookmarks=true,
    urlcolor=webbrown, citecolor=Black, linkcolor=Black% Link colors
}}

\pagestyle{plain}
\title{\textbf{Quickselect - Analysis of the expected number of comparisons}}
\author{{Alberto Parravicini}}
\date{}	% default \today

% =============================================== BEGIN

\begin{document}
\maketitle
\pagenumbering{roman}
\setcounter{page}{1}

\section{Abstract}
The following report will analyze the number of comparisons performed by a \textbf{Rust} implementation of the \textbf{Quickselect} algorithm, a randomized algorithm used to find the $k-th$ smallest element in an unordered list.

The first section of the report presents the algorithm, and its \textbf{Rust implementation}.

The second section studies the number of \textbf{comparisons} between done by the algorithm,
i.e. how many elements from the input list must be compared to each other to give the desired output.
This numbers are then compared to the theoretical expected number of comparisons done by the algorithm.\newline
The algorithm is tested against vectors with increasing size $n$ and various ranges of $k$. 

As addendum, an analysis of the \textbf{execution time} of \textit{Quickselect} is reported, to show how the number of comparisons influences the running time of the algorithm.

\section{Quickselect's algorithm implementation}
\vspace{-5mm}
\subsection{Theoretical background}
\vspace{-5mm}
Given a list of $\mathbf{n}$ elements for which exists a total order under $\mathbf{\leq}$, the \textbf{Quickselect} algorithm is used to find the $\mathbf{k-th}$ smallest element in the list (also referred as element of \textit{rank k}), for $\mathbf{k \in [1,\ n]}$.

\newpage
To select the element of rank $k$ (with $1 \leq k \leq n$), the algorithm works as follow:

\texttt {\textcolor{Maroon}{quickselect(list, start, end)}: \newline
	\null\quad\quad pick an element at random from the list, \newline
    \null\quad\quad\quad\quad in the range [start, end], call it \textbf{pivot}; \newline
	\null\quad\quad put all the elements $<$ pivot on its left; \newline
	\null\quad\quad put all the elements $>$ pivot on its right; \newline
    \null\quad\quad the pivot is now in its sorted position in the list: \newline
    \null\quad\quad\quad\quad if \textcolor{RoyalBlue}{position(pivot) $==$ k}, return it; \newline
    \null\quad\quad\quad\quad if \textcolor{RoyalBlue}{position(pivot) $<$ k}, return \textcolor{Maroon}{quickselect(start, position(pivot))}; \newline
    \null\quad\quad\quad\quad if \textcolor{RoyalBlue}{position(pivot) $>$ k}, return \textcolor{Maroon}{quickselect(position(pivot), end)}; \newline}

It can be seen how there is a close resemblance between the \textbf{Quickselect} and the \textbf{Quicksort} algorithms. \\
Indeed, \textbf{Quickselect} can be seen as a modified version of \textbf{Quicksort} in which the recursion is applied only to the portion of the list where the desired element of \textit{rank k} is expected to be found.

\vspace{-5mm}
\subsection{Rust implementation}
\vspace{-5mm}
\textit{Rust} was chosen as the programming language of the implementation as the inherent speed of the language, combined with the relative simplicity of the \textbf{Quickselect} algorithm, make it a good candidate for performing a large number of tests over many different parameter values. 

What follows are snippets of code, to explain how the \textbf{Quickselect} algorithm was implemented:

$\bullet$ \textbf{Quickselect}
\begin{minted}[baselinestretch=1, fontsize=\footnotesize]{rust}
fn quickselect(vec: &mut Vec<usize>, 
               start: usize, end: usize, 
               k: usize, num_of_comparisons: u32) -> (usize, u32) {
    let mut num_of_comparisons: u32 = num_of_comparisons;

    if k < start || k > end {
        panic!("INVALID VALUE OF K: {}", k)
    }
    // If just one element is present, return it;
    if start >= end {
        return (vec[end], num_of_comparisons)
    }
    let pivot_index = rand::thread_rng().gen_range(start, end + 1);
    // Put the current pivot in its correct place, and return its position;
    let (pivot_index, temp_num_of_comparisons) = partition(vec, start, end, pivot_index);
    num_of_comparisons += temp_num_of_comparisons;

    // Apply the sorting only where the desired element could be,
    // or return it if it was found;
    if k == pivot_index {
        return (vec[k], num_of_comparisons)
    }
    else if k < pivot_index {
        let result = quickselect(vec, start, pivot_index, k, num_of_comparisons);
        return result
    }
    else {
        let result = quickselect(vec, pivot_index + 1, end, k, num_of_comparisons);
        return result
    }
}
\end{minted}

$\bullet$ \textbf{Select a pivot and put it into its sorted position}
\begin{minted}[baselinestretch=1, fontsize=\footnotesize]{rust}
fn partition(vec: &mut Vec<usize>, 
             start: usize, end: usize,
             pivot_index: usize) -> (usize, u32) {
    let pivot_value = vec[pivot_index];
    let mut temp_num_of_comparisons: u32 = 0;

    // Temporarily put the pivot at the end of the vector;
    vec.swap(pivot_index, end);
    let mut store_index = start;
    for i in start..end {
        temp_num_of_comparisons += 1;
        if vec[i] < pivot_value {
            // If a value lower than the pivot is found, put it in the left part of the vector;
            vec.swap(store_index, i);
            // store_index keeps track of how many values lower that the pivot exist,
            // and where the pivot should be placed at the end;
            store_index += 1;
        }
    }
    // Put the pivot in its correct place;
    vec.swap(end, store_index);

    // Return the sorted position of the pivot and the number of comparisons performed;
    (store_index, temp_num_of_comparisons)
}

\end{minted}

\section{Analysis of the expected number of comparisons}
\vspace{-5mm}
\subsection{Theoretical background}
\vspace{-5mm}
For any two elements $X_i,\ X_j$ of the list, with $j > i$, \\
\[
    X_{i,j} = \begin{cases}
                    1\ if\ X_i,\ X_j\ are\ compared\ during\ the\ execution\ of\ the\ algorithm. \\
                    0\ else.
                \end{cases}
\]
The complexity of \textbf{Quickselect} is related to the total number of comparisons $X$ performed by the algorithm.\\
It can be shown that the expected number of comparisons $E[X]$ is given by:
$$E[X] = E\Big[\sum_{i < j}{X_{i,j}}\Big] = \Big(2n + 2n \cdot\ln\Big(\frac{n}{n - k}\Big) + 2k \cdot\ln\Big(\frac{n - k}{k}\Big)\Big)\cdot (1 + o(1)) \quad\quad (\ast)$$
\vspace{-5mm}
A number of observations can be done on this formula:
\begin{itemize}
    \item If $k$ is chosen to be proportional to $n$, the number of comparisons becomes linear with respect to $n$.\\
    As an example, if $k = \frac{n}{2}$, \textcolor{Maroon}{$E[X] = n \cdot (2 + 2 \cdot \ln(2)) \cdot (1 + o(1))$}.

    \item The function $(\ast)$ is not defined for $k = n$; that said, it can be seen that for $k = n$ the value of $(\ast)$ is very close to its value in $k = 0$. \\
        As such, the function will be approximated in this way in the following analyses.

    \item In the interval $k \in [1, n)$, it can be seen that $(\ast)$ has a single local maximum, in $k = \frac{n}{2}$. In fact, by putting the derivative of $(\ast)$ w.r.t. $k$ equal to $0$: $$\frac{\partial}{\partial k}\Big(2n + 2n \cdot\ln\Big(\frac{n}{n - k}\Big) + 2k \cdot\ln\Big(\frac{n - k}{k}\Big)\Big) = \ln\Big(\frac{n}{k} - 1 \Big) = 0$$
    From which one can find \textcolor{Maroon}{$k = \frac{n}{2}$}, which is a maximum as the second derivative is negative in the interval $k \in [1, n)$.
    Moreover, the local minima of $(\ast)$ are at the extremes of the range of $k$.
\end{itemize}

The \textbf{Quicksort} algorithm has been tested against lists of increasing size, while keeping the value of $k$ proportional to the size of the list, and against different values of $k$, while keeping the list size fixed.
Note that as the selection of the pivot is random, the permutation of the input list doesn't influence the number of comparisons performed by the algorithm; as such, it is possible to use already sorted list as input of the algorithm.

\subsection{\textbf{First test:} increasing size of the list, select the median}
The first test consisted in running Karger's algorithm against connected graphs with \textit{increasing number of vertices and edges}.
The number of vertices goes from $20$ to $100$ (with a step size of $10$ vertices), and the number of edges of each graph is equal to $4 \cdot num\_vertices$.
For each graph, Karger's algorithm was run $50$ times. For each graph size, $5$ different graphs were built
(thus, $9 \cdot 50 \cdot 5$ iterations of the algorithm have been performed in this test).

The success rate of the algorithm with respect to each graph size has been compared to the theoretical success rate baseline.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, center, keepaspectratio=1]{{"increasing_size"}.pdf}
    \caption{\emph{In {\color{RoyalBlue} blue}, the measured number of comparisons. In {\color{red}red}, the theoretical number of comparisons.}}
\end{figure}

It can be seen that if the ratio between number of edges and vertices is kept constant, increasing the number of vertices doesn't hinder
the success rate of Karger's algorithm. In all cases, the verified success rate is much higher than the theoretical lower bound,
which could be related to the low value of the ratio between number of edges and vertices.

\subsection{\textbf{Second test:} increasing value of k, fixed list size}
From the previous test, it was empirically shown that as long as the ratio between number of edges and vertices is constant,
Karger's algorithm success rate isn't affected by the size of the graph.
On the other hand, increasing the number of edges while keeping the number of vertices fixed could have an impact on the algorithm success percentage.

To verify this hypothesis, Karger's algorithm was run on connected graphs with 50 vertices and a number of edges ranging from $50$ to $500$ (with a step size of $50$ edges).
For each number of edges, $5$ different graphs were built. On each of them, the algorithm was run $50$ times, for a total of $10 \cdot 5 \cdot 50$ iterations of the algorithm.
Once again, the results are compared to the theoretical success rate baseline.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, center, keepaspectratio=1]{{"increasing_rank"}.pdf}
    \caption{\emph{In {\color{RoyalBlue} blue}, the measured number of comparisons. In {\color{red}red}, the theoretical number of comparisons.}}
\end{figure}

It can be seen that increasing the number of edges while keeping the number of vertices fixed dramatically reduces the algorithm success rate,
to the point where the theoretical lower bound of the success rate is almost reached. \\

This can be intuitively explained as follows: Karger's algorithm will output the correct minimum cut size as long as no edge belonging to the minimum cut is contracted. \\
However, increasing the number of edges while keeping fixed the number of vertices implies that the graph will have many multi-edges.
Clearly, if an edge between two vertices belongs to the minimum cut, all edges between these two edges will also belong to the minimum cut. \\
If we have a multi-edge between two nodes, and this multi-edge belongs to the minimum cut, the probability of contracting it (which would lead to a fail of Karger's algorithm) will be higher that the probability
of selecting a single edge.

It can also be noted that if $|V|$ = $|E|$, Karger's algorithm always give the correct minimum cut value (i.e. $1$): this happens because the graph is a tree, and contracting the edges will never form a multi-edge. Hence, after $n - 2$ iterations, the graph will always have just $1$ edge left.

\section{Addendum}

\subsection{Execution time analysis}
By making use of the \textit{microbenchmark} library, it is possible to monitor the execution time of the implementation of Karger's algorithm.\\
Even though the implementation is not optimized for fast execution times, it is still interesting to study how the execution time scales with respect to the number of nodes and edges of a graph.\\

For both the previous tests, execution times were recorded and summarized in the following plots.

It emerges a linear scaling with respect to the number of vertices and edges.\\
This empirical results are coherent with the theoretical ones if we assume that the contraction operation happens in constant time.\\
Even without knowing the details of \textit{igraph}, it is clear that contracting an edge requires a loop whose number of steps is equal to the size of the neighbourhood of a given vertex (i.e. at most $|V|$). \\
This would give a theoretical complexity of $O(|V|^2)$; however, if the graph is rather sparse, it might happen that the size of the neighbourhood of a given vertex is quite small, and thus the contraction operation happens quite fast in practice.

In the second plot, where the number of vertices is kept constant, this behaviour emerges even more: having a constant number of vertices would imply a constant execution time, but in practice it can be noticed once again a linear scaling; this happens because the graph becomes progressively less sparse, and thus the contraction operation becomes more and more time consuming.

\vspace{-10mm}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth, center, keepaspectratio=1]{{"increasing_size_time"}.pdf}
    \caption{\emph{In {\color{RoyalBlue} blue}, the measured execution time. In {\color{red}red}, the linear regression of the execution time.}}
\end{figure}
\vspace{-5mm}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth, center, keepaspectratio=1]{{"increasing_rank_time"}.pdf}
    \caption{\emph{In {\color{RoyalBlue} blue}, the measured execution time. In {\color{red}red}, the order 2 polynomial regression of the execution time.}}
\end{figure}

\vspace{20mm}
$\bullet$ \textbf{Full code (Rust implementations + R profiling) available at \newline \href{https://github.com/AlbertoParravicini/data_structures_and_algorithms}{https://github.com/AlbertoParravicini/data\_structures\_and\_algorithms}}

\end{document}

