\documentclass[
12pt,
a4paper,
oneside,
headinclude,
footinclude]{article}



\usepackage[table,xcdraw,svgnames]{xcolor}
\usepackage[capposition=bottom]{floatrow}
\usepackage[colorlinks]{hyperref} % to add hyperlinks
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{csquotes}
\usepackage[export]{adjustbox}[2011/08/13]
% \usepackage{subfig}
\usepackage{array}
\usepackage{url}
\usepackage{graphicx} % to insert images
\usepackage{titlepic} % to insert image on front page
\usepackage{geometry} % to define margin
\usepackage{listings} % to add code
\usepackage{caption}
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage[utf8]{inputenc} % Required for including letters with accents
\usepackage{color}
\usepackage{subcaption}
\usepackage[nochapters, eulermath, dottedtoc ]{classicthesis}
\usepackage{listings} % For R code

\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}


\lstset{frame=tb,
  language=R,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3,
  frame=none
}

\definecolor{webbrown}{rgb}{.6,0,0}

\usepackage{titlesec} % to customize titles
\titleformat{\chapter}{\normalfont\huge}{\textbf{\thechapter.}}{20pt}{\huge\textbf}[\vspace{2ex}\titlerule] % to customize chapter title aspect
\titleformat{\section} % to customize section titles
  {\fontsize{14}{15}\bfseries}{\thesection}{1em}{}

\titlespacing*{\chapter}{0pt}{-50pt}{20pt} % to customize chapter title space

\graphicspath{ {../Figures/} } % images folder
\parindent0pt \parskip10pt % make block paragraphs
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=3cm,rmargin=3cm,headheight=3cm,headsep=3cm,footskip=1cm} % define margin
\hyphenation{Fortran hy-phen-ation}

\AtBeginDocument{%
    \hypersetup{
    colorlinks=true, breaklinks=true, bookmarks=true,
    urlcolor=webbrown, citecolor=Black, linkcolor=Black% Link colors
}}

\pagestyle{plain}
\title{\textbf{Karger's randomized minimum cut algorithm - An analysis of its success frequency}}
\author{{Alberto Parravicini}}
\date{}	% default \today

% =============================================== BEGIN

\begin{document}
\maketitle
\pagenumbering{roman}
\setcounter{page}{1}

\section{Abstract}
The following report will analyzes the performances, in terms of success frequency, of an R implementation of \textbf{Karger's algorithm} to compute
the minimum cut of a graph by means of a randomized procedure.

The first section of the report briefly presents the algorithm, and its \textbf{R implementation}.

The second section studies the \textbf{success rate} of the algorithm,
i.e. how many times Karger's algorithm gives the same result as a deterministic algorithm;
the success rate of Karger's algorithm is then compared to the theoretical minimum success rate of the algorithm.\newline
The algorithm is tested against a number of connected graphs with increasing number of nodes and edges,
to get a better understanding of how the structure of a graph influences the performances of the algorithm.


\section{Karger's algorithm implementation}
\vspace{-5mm}
\subsection{Theoretical background}
\vspace{-5mm}
Karger's algorithm is employed to compute the \textbf{minimum cut} of a graph, i.e. the smallest number of edges that can be removed from a given graph without losing the edge connectivity of the graph.

Given a graph with n vertices, the algorithm works as follow:

\texttt {for i in \{1,2, .. , n - 2\}: \newline
	\null\quad\quad pick an edge e at random from the graph \newline
	\null\quad\quad contract the edge e (i.e. merge its endpoints) \newline
return the size of the set of edges that connects the two remaining vertices.}


\subsection{Notes}
\vspace{-5mm}
Self loops aren't allowed in the graph: if an edge contraction produces self loops, they are removed. \\
The following analyses assume a \textit{connected graph}. Whether a graph is disconnected or not (and thus has a minimum cut of $0$) can be determined deterministically
with algorithms whose complexity is lower than Karger's algorithm, for example using a \textbf{breadth first search} (time complexity $O(|V| + |E|))$.

\subsection{R implementation}
\textit{R} was chosen as the programming language of the implementation as the focus of the report isn't on the speed and computational efficiency of the implementation,
but on the analysis of the success rate of the algorithm. \\
\textit{R} offers a robust graph library called \textit{igraph}, which in turns is a wrapper to the homonymous \textit{C} library.
That said, \textit{igraph} doesn't provide a way to reliably create a random connected graph and to contract edges in an easy way,
so it was necessary to implement these functionality by hand.


What follows are snippets of code, to explain how Karger's algorithm was implemented:


$\bullet$ \textbf{Generate a random connected graph}

\begin{lstlisting}
make_random_connected_graph <- function(num_vertices, num_edges) {
  g <- make_empty_graph(directed = F)
  g <- g + vertex("1")
  # Add a new vertex, connect it to a random existing vertex.
  # Repeat the process num_vertices times.
  for(i in 2:num_vertices) {
    random_vertex <- V(g)[sample(1:length(V(g)), 1)]
    g <- g + vertex(as.character(i))
    g <- g + edge(i, random_vertex)
  }

  # Add the remaining num_edges - num_vertices edges.
  for(j in 1:(num_edges - num_vertices + 1)) {
    # Select two random, distinct vertices.
    random_vertices <- V(g)[sample(1:length(V(g)), 2, replace = F)]
    g <- g + edge(random_vertices[1], random_vertices[2])
  }
  return(g)
}
\end{lstlisting}

\newpage
$\bullet$ \textbf{Karger's randomized minimum cut algorithm}
\begin{lstlisting}
karger_random_min_cut <- function(g) {
  num_vertices = length(V(g))

  for(i in 1:(num_vertices - 2)) {
    # Select a random edge, and keep its endpoints.
    random_edge <- E(g)[sample(1:length(E(g)), 1)]
    from_vertex <- ends(g, random_edge)[[1]]
    to_vertex <- ends(g, random_edge)[[2]]

    # Contract the selected edge.
    edges_to_be_changed <- neighbors(g, to_vertex)
    for (j in 1:length(edges_to_be_changed)) {
      g <- g + edge(from_vertex, V(g)[edges_to_be_changed[j]]$name)
    }
    g <- delete.vertices(g, to_vertex)
	# Remove self loops from the graph.
    g <- simplify(g, remove.multiple = F, remove.loops = T)
  }
  # Return the number of edges, as our randomized min_cut
  return(length(E(g)))
}
\end{lstlisting}

\section{Analysis of the success frequency}

\subsection{Theoretical background}

It can be proved that, given a graph with n vertices, Karger's algorithm will
return the correct minimum cut of the graph with probability greater or equal to $\frac{2}{n \cdot (n - 1)}$.
This is the probability of never contracting an edge belonging to the minimum cut of the graph,
and assuming that the graph has the minimum possible number of edges, equal to $\frac{n \cdot k}{2}$, where $k$ is the size of the minimum cut.

Moreover, it can be shown that executing the algorithm $n \cdot (n - 1) \cdot \ln{n}$ times, and keeping the smallest cut found,
gives an upper bound on the error probability equal to $\frac{1}{n^{2}}$.

The performances of the previous implementation of Karger's algortihm have been analyzed against graphs with increasing number of vertices and edges,
and against graphs with fixed number of vertices and increasing number of edges.
As mentioned previously, all graphs are connected, as using Karger's algorithm isn't an efficient tool to determine if a graph is edge connected.

The \textit{igraph} library provides a function to deterministically determine the minimum cut of a graph.
This function was used to check the correctness of each run of Karger's algorith.

\subsection{\textbf{First test:} increasing number of vertices and edges}
The first test consisted in running Karger's algorithm against connected graphs with \textit{increasing number of vertices and edges}.
The number of vertices goes from $20$ to $100$ (with a step size of $10$ vertices), and the number of edges of each graph is equal to $4 \cdot num\_vertices$.
For each graph, Karger's algorithm was run $50$ times. For each graph size, $5$ different graphs were built
(thus, $9 \cdot 50 \cdot 5$ iterations of the algorithm have been performed in this test).

The success rate of the algorithm with respect to each graph size has been compared to the theoretical success rate baseline.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, center, keepaspectratio=1]{{"KARGER_vertices_edges"}.pdf}
    \caption{\emph{In {\color{RoyalBlue} blue}, the success frequency. In {\color{red}red}, the baseline success frequency.}}
\end{figure}

It can be seen that if the ratio between number of edges and vertices is kept constant, increasing the number of vertices doesn't hinder
the success rate of Karger's algorithm. In all cases, the verified success rate is much higher than the theoretical lower bound,
which could be related to the low value of the ratio between number of edges and vertices.

\subsection{\textbf{Second test:} increasing number of edges, fixed number of nodes}
From the previous test, it was empirically shown that as long as the ratio between number of edges and vertices is constant,
Karger's algorithm success rate isn't affected by the size of the graph.
On the other hand, increasing the number of edges while keeping the number of vertices fixed could have an impact on the algorithm success percentage.

To verify this hypothesis, Karger's algorithm was run on connected graphs with 50 vertices and a number of edges ranging from $50$ to $500$ (with a step size of $50$ edges).
For each number of edges, $5$ different graphs were built. On each of them, the algorithm was run $50$ times, for a total of $10 \cdot 5 \cdot 50$ iterations of the algorithm.
Once again, the results are compared to the theoretical success rate baseline.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, center, keepaspectratio=1]{{"KARGER_edges"}.pdf}
    \caption{\emph{In {\color{RoyalBlue} blue}, the success frequency. In {\color{red}red}, the baseline success frequency.}}
\end{figure}

It can be seen that increasing the number of edges while keeping the number of vertices fixed dramatically reduces the algorithm success rate,
to the point where the theoretical lower bound of the success rate is almost reached. \\

This can be intuitively explained as follows: Karger's algorithm will output the correct minimum cut size as long as no edge belonging to the minimum cut is contracted. \\
However, increasing the number of edges while keeping fixed the number of vertices implies that the graph will have many multi-edges.
Clearly, if an edge between two vertices belongs to the minimum cut, all edges between these two edges will also belong to the minimum cut. \\
If we have a multi-edge between two nodes, and this multi-edge belongs to the minimum cut, the probability of contracting it (which would lead to a fail of Karger's algorithm) will be higher that the probability
of selecting a single edge.

It can also be noted that if $|V|$ = $|E|$, Karger's algorithm always give the correct minimum cut value (i.e. $1$): this happens because the graph is a tree, and contracting the edges will never form a multi-edge. Hence, after $n - 2$ iterations, the graph will always have just $1$ edge left.

\section{Addendum}

\subsection{Execution time analysis}
By making use of the \textit{microbenchmark} library, it is possible to monitor the execution time of the implementation of Karger's algorithm.\\
Even though the implementation is not optimized for fast execution times, it is still interesting to study how the execution time scales with respect to the number of nodes and edges of a graph.\\

For both the previous tests, execution times were recorded and summarized in the following plots.

It emerges a linear scaling with respect to the number of vertices and edges.\\
This empirical results are coherent with the theoretical ones if we assume that the contraction operation happens in constant time.\\
Even without knowing the details of \textit{igraph}, it is clear that contracting an edge requires a loop whose number of steps is equal to the size of the neighbourhood of a given vertex (i.e. at most $|V|$). \\
This would give a theoretical complexity of $O(|V|^2)$; however, if the graph is rather sparse, it might happen that the size of the neighbourhood of a given vertex is quite small, and thus the contraction operation happens quite fast in practice.

In the second plot, where the number of vertices is kept constant, this behaviour emerges even more: having a constant number of vertices would imply a constant execution time, but in practice it can be noticed once again a linear scaling; this happens because the graph becomes progressively less sparse, and thus the contraction operation becomes more and more time consuming.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, center, keepaspectratio=1]{{"EXEC_TIME_vertices_edges"}.pdf}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, center, keepaspectratio=1]{{"EXEC_TIME_edges"}.pdf}
\end{figure}

\end{document} 

